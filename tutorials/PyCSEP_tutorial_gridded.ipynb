{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridded forecast model tutorial with Italy examples\n",
    "\n",
    "In this tutorial, we will load and test grid-based forecasts and interpret the results of the tests provided in the PyCSEP package for gridded forecasts. We will work with two time-independent grid-based forecasts submitted as part of the CSEP Italy testing experiment (see [Werner et al, 2010](https://doi.org/10.4401/ag-4840), [Taroni et al, 2018](https://doi.org/10.1785/0220180031) for some previous testing results). Our goal is to compare the performance of these two forecasts for describing observed Italian seismicity.   \n",
    "\n",
    "This is essentially a three step process:  \n",
    "    1. Read in (and plot) a gridded forecast  \n",
    "    2. Set up an evaluation catalog of observed events  \n",
    "    3. Run PyCSEP tests and interpret the results    \n",
    "\n",
    "\n",
    "We introduce the concepts to the reader and encourage them to explore the other tests available. Full documentation of the package can be found [here](https://docs.cseptesting.org/) and any issues can be reported on the [PyCSEP Github page](https://github.com/SCECcode/pycsep)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most of the core functionality can be imported from the top-level csep package. \n",
    "# Utilities are available from the csep.utils subpackage.\n",
    "import csep\n",
    "from csep.core import regions, catalog_evaluations, poisson_evaluations as poisson\n",
    "#from csep.core import poisson_evaluations as poisson\n",
    "from csep.utils import datasets, time_utils, comcat, plots, readers\n",
    "\n",
    "import numpy\n",
    "## Cartopy required for updated plots\n",
    "import cartopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to start by setting up some experiment parameters. It is good practice to set this up early. Note, the start and end date of the forecast should be chosen based on the creation of the forecast. This is important for time-independent forecasts because they can be rescaled to any arbitrary time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up experiment parameters\n",
    "start_date = time_utils.strptime_to_utc_datetime('2010-01-01 00:00:00.0')\n",
    "end_date = time_utils.strptime_to_utc_datetime('2015-01-01 00:00:00.0')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These forecasts are included with the main repository in the case of the Werner forecast and the tutorial repository for the Meletti forecast. You can learn more about the format of gridded forecasts [in the documentation](https://docs.cseptesting.org/concepts/forecasts.html). The filepath is relative to the root directory of the package, so you can specify any file location for your forecasts. You can also attach a name, which is very useful for comparisons later if you chose something sensible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loads from the PyCSEP package\n",
    "werner_forecast = csep.load_gridded_forecast(datasets.hires_ssm_italy_fname,\n",
    "                                             name='Werner, et al (2010)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You may need to edit the file location here depending on your set up\n",
    "meletti_forecast = csep.load_gridded_forecast(\"../workshop_data/forecasts/meletti.MPS04after.italy.5yr.2010-01-01.dat\",\n",
    "                                              name =\"Meletti et al (2010), MPS working group\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gridded forecasts inherit the region from the forecast, so there is no requirement to explicitly set this. We should check, however, that the forecast regions for catalogs we want to compare are the same so that they are testable with a single catalog. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sanity check - if catalogs have the same region this will provide no output.\n",
    "numpy.testing.assert_allclose(meletti_forecast.region.midpoints(), werner_forecast.region.midpoints())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualise this forecast, we will use `forecast.plot()` with some specifications to get a nicer looking figure. We will do this by creating a dictionary containing the plot arguments.  \n",
    "\n",
    "These arguments are, in order:\n",
    "\n",
    "    - Assign a title\n",
    "    - Set labels to the geographic axes\n",
    "    - Draw country borders\n",
    "    - Set a linewidth of 0.5 to country borders\n",
    "    - Select ESRI Imagery as a basemap.\n",
    "    - Assign 'rainbow' as colormap. Possible values from from matplotlib.cm library\n",
    "    - Defines 0.8 for an exponential transparency function (default is 0 for constant alpha, whereas 1 for linear).\n",
    "    - An object cartopy.crs.Projection() is passed as Projection to the map  \n",
    " \n",
    " The complete description of plot arguments can be found in `csep.utils.plots.plot_spatial_dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {'title': 'Italy 10 year forecast',\n",
    "             'grid_labels': True,\n",
    "             'borders': True,\n",
    "             'feature_lw': 0.5,\n",
    "             'basemap': 'ESRI_imagery',\n",
    "             'cmap': 'rainbow',\n",
    "             'alpha_exp': 0.8,\n",
    "             'projection': cartopy.crs.Mercator()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The map extent can also be defined. Otherwise, the extent of the data would be used. The dictionary defined must be passed as an argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = werner_forecast.plot(extent=[3, 22, 35, 48],\n",
    "                   show=True,\n",
    "                   plot_args=args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the second forecast\n",
    "# An excercise for the reader\n",
    "ax = ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up evaluation catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to import the observed catalog that we want to use to test the forecast - we call this the evaluation catalog.  \n",
    "\n",
    "There are multiple ways to read in evaluation catalogs, including reading directly from ComCat for US models. There are also various readers currently included with the package, including those for JMA and the INGV HORUS catalogs. These functions are in `/csep/utils/readers.py` if you would like to see them or understand how to add your own.  \n",
    "\n",
    "In this case we demonstrate using the European RCMT catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load catalog\n",
    "italy_test_catalog = csep.load_catalog(\"../workshop_data/catalogs/europe_rcmt_2010-2015.csv\",\n",
    "                                       type=\"ingv_emrcmt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the catalog to check the range of dates, locations and magnitudes of the events in the evaluation catalog, as well as the total number of events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also filter the catalog to the desired time-space-magnitude range. Crucially, we must also filter the catalog to the forecast region in order to carry out any testing in the next step. This is also why we checked that the forecasts we want to compare are in the same spatial region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italy_test_catalog = italy_test_catalog.filter_spatial(werner_forecast.region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our magnitude range is not consistent with the parameters we established earlier, so we have to filter for magnitude also. This is obviously important to fairly test a forecast, and you can see why if you re-run this tutorial without this step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italy_test_catalog = italy_test_catalog.filter('magnitude >= 4.95')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have run the above code, you should be left with a catalog of 13 events. Print the catalog with the standard python `print` command to check the range of dates, locations and magnitudes of the events in the evaluation catalog, as well as the total number of events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run consistency tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we wish to answer some questions about our forecasts and their performance. In this example, we will investigate the spatial properties of the forecast models and how well the forecasts describe the observed spatial distribution of seismicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The consistency tests implemented for gridded forecasts in PyCSEP are the N, S and M-test described in [Schorlemmer et al, 2007](https://doi.org/10.1785/gssrl.78.1.17) and [Zechar et al, 2010](https://doi.org/10.1785/0120090192). These are located in the `poisson_evaluations` file that we have imported as `poisson`. To carry out a test, we simply provide the forecast we wish to test and an evaluation forecast. The spatial test requires simulating from the Poisson forecast to provide uncertainty. The verbose option prints the status of the simulations to the standard output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_test_result_werner = poisson.spatial_test(werner_forecast, italy_test_catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Repeat the spatial test for our second example forecast\n",
    "spatial_test_result_meletti = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyCSEP provides easy ways of storing objects to a JSON format using csep.write_json(). The evaluations can be read back into the program for plotting using `csep.load_evaluation_result()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to write to .json file (optional)\n",
    "## You can look at the contents of this file in jupyter lab to see how the data is stored\n",
    "csep.write_json(spatial_test_result_meletti, 'example_spatial_test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot these results using the `plot_poisson_consistency_test` function from the `plots` file of `csep.utils`, where you can find more details on the plot arguments. Again, we use a dictionary to set up some plot arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'figsize': (6,5),\n",
    "        'title': r'$\\mathcal{S}-\\mathrm{test}$',\n",
    "        'title_fontsize': 18,\n",
    "        'xlabel': 'Log-likelihood',\n",
    "        'xticks_fontsize': 12,\n",
    "        'ylabel_fontsize': 12,\n",
    "        'linewidth': 1,\n",
    "        'capsize': 4,\n",
    "        'hbars':True,\n",
    "        'tight_layout': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now going to plot the results of both forecasts for comparison. We set `one_sided_lower=True` as usual for an L-test, where the model is rejected if the observed is located within the lower tail of the simulated distribution. We can supply multiple `spatial_test_result` objects in a list (specified in the square brackets as standard in python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plots.plot_poisson_consistency_test([spatial_test_result_werner, spatial_test_result_meletti],\n",
    "                                         one_sided_lower=True, plot_args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us something about the spatial performance of these models. We can repeat this process for N or M tests using the `number_test` or `magnitude_test` from `poisson_evaluations` if we are more interested in these components specifically, or getting a fuller picture of where the forecast does well and not so well.   \n",
    "Try out a `likelihood_test` or `conditional_likelihood_test` (also from `poisson_evaluations`). What does this tell you about the two forecasts?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare forecast test results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have test results for two different forecasts, we want to compare their performance in terms of their information gain. We will implement this using the T-test, which [Rhoades et al, 2011](https://link.springer.com/article/10.2478/s11600-011-0013-5) describe as a better method of directly comparing likelihoods than the above consistency tests.  \n",
    "The paired T-test compares a forecast to a base model, in this case we have chosen to use `meletti_forecast` as the baseline and any other models are compared to it. As the 'paired' part implies, the paired T-test always takes two forecast arguments, though we can run it for multiple different models by repeating the call, updating the first forecast argument and holding the second one fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_test1 = poisson.paired_t_test(werner_forecast, meletti_forecast, italy_test_catalog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `plot_comparison_test` function is used here, which works similarly to the other `plots` functions. Again, we can set up our plot arguments with a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_args = {'title': 'Paired T-test result',\n",
    "             'ylabel': 'Information gain',\n",
    "             'xlabel': 'Model'}\n",
    "\n",
    "ax = plots.plot_comparison_test([paired_test1],\n",
    "                                 plot_args= comp_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How should this result be interpreted? Are there other tests that might be useful here? What are your overall conclusions on the performance of these two forecasts?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are happy with all of this so far, you can try:  \n",
    "    - Changing the time period for the evaluation    \n",
    "    - Filtering the evalutaion catalog by a different time period or magnitude    \n",
    "    - Using PyCSEP gridded-forecast tests on your own forecast models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References  \n",
    "\n",
    "Rhoades, D. A., D. Schorlemmer, M. C. Gerstenberger, A. Christophersen, J. D. Zechar, and M. Imoto (2011). Efficient testing of earthquake forecasting models, Acta Geophys 59 728-747.    \n",
    "Schorlemmer, D., M. Gerstenberger, S. Wiemer, D. D. Jackson, and D. A. Rhoades (2007). Earthquake likelihood model testing, Seismological Research Letters 78 17-29.  \n",
    "Taroni, M., W. Marzocchi, D. Schorlemmer, M. J. Werner, S. Wiemer, J. D. Zechar, L. Heiniger, F. Euchner (2018). Prospective CSEP Evaluation of 1‐Day, 3‐Month, and 5‐Yr Earthquake Forecasts for Italy, Seismological Research Letters (2018) 89 (4): 1251–1261  \n",
    "Werner, M. J., J. D. Zechar, W. Marzocchi, S., CSEP-Italy Working Group (2010). Retrospective evaluation of the five-year and ten-year CSEP-Italy earthquake forecasts, Annals of Geophysics, 53, 3.  \n",
    "Zechar, J. D., M. C. Gerstenberger, and D. A. Rhoades (2010). Likelihood-Based Tests for Evaluating Space-Rate-Magnitude Earthquake Forecasts, Bulletin of the Seismological Society of America 100 1184-1195."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}